{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam: Exercise\n",
    "\n",
    "For this exercise we will be build Adam up from scratch starting with regular gradient descent. We will also be utilizing mini batches to introduce stochasticity to the optimization. \n",
    "\n",
    "We will be working with the mnist_784 data set and a simple shallow neural network. \n",
    "\n",
    "If you do not have scikit-learn then you can get it here: https://scikit-learn.org/stable/install.html\n",
    "\n",
    "This code is heavily inspired by Doug’s code from CSCI 447/547 lecture 05_multilayer_perceptron.\n",
    "\n",
    "First we need to get the data, define the network and define some functions to perform on the data. You don’t need to do anything with this first block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# In order to run this in class, we're going to reduce the dataset by a factor of 5\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X[::5]\n",
    "y = y.astype(int)[::5]\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "\n",
    "# Here we specify the size of our neural network.\n",
    "# We are mapping from 784 to 10 with 256 hiden layer nodes.\n",
    "\n",
    "m = len(X)\n",
    "n_0 = 784\n",
    "n_1 = 256\n",
    "N = 10\n",
    "\n",
    "\n",
    "# Function to convert categorical labels into one-hot matrix.\n",
    "def convert_to_one_hot(y, n_classes):\n",
    "    T = np.zeros((y.shape[0], n_classes))\n",
    "    for t, yy in zip(T, y):\n",
    "        t[yy] = 1\n",
    "    return T\n",
    "\n",
    "\n",
    "# Convert the data to one hot notation\n",
    "one_hot_y_actual = convert_to_one_hot(y, N)\n",
    "one_hot_y_test = convert_to_one_hot(y_test, N)\n",
    "\n",
    "\n",
    "# Sigmoid function (activation)\n",
    "def sigmoid(a):\n",
    "    return 1. / (1 + np.exp(-a))\n",
    "\n",
    "\n",
    "# Softmax function (final layer for classification)\n",
    "def softmax(A):\n",
    "    numerator = np.exp(A)\n",
    "    denominator = numerator.sum(axis=1)\n",
    "    return numerator / denominator[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Categorical cross-entropy\n",
    "def L(T, S, W1, W2, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    return -1. / len(T) * np.sum(T * np.log(S)) + np.sum(0.5 * alpha_1 * W1 ** 2) + np.sum(0.5 * alpha_2 * W2 ** 2)\n",
    "\n",
    "\n",
    "# Run the neural network forward, given some weights and biases\n",
    "def feedforward(X, W1, W2, b1, b2):\n",
    "    # Feedforward\n",
    "    A1 = X @ W1 + b1\n",
    "    Z1 = sigmoid(A1)\n",
    "    A2 = Z1 @ W2 + b2\n",
    "    y_pred = softmax(A2)\n",
    "    return y_pred, Z1\n",
    "\n",
    "\n",
    "# Compute the neural network gradients using backpropagation\n",
    "def backpropogate(y_pred, Z1, X, y_obs, alpha_1=1e-2, alpha_2=1e-5):\n",
    "    # Backpropogate\n",
    "    delta_2 = (1. / len(y_pred)) * (y_pred - y_obs)\n",
    "    grad_W2 = Z1.T @ delta_2 + alpha_2 * W2\n",
    "    grad_b2 = delta_2.sum(axis=0)\n",
    "\n",
    "    delta_1 = delta_2 @ W2.T * Z1 * (1 - Z1)\n",
    "    grad_W1 = X.T @ delta_1 + alpha_1 * W1\n",
    "    grad_b1 = delta_1.sum(axis=0)\n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2\n",
    "\n",
    "\n",
    "def mini_batch(x_sample, y_sample, start_batch_size):\n",
    "    \"\"\"\n",
    "    Takes a copy of x_sample and y_sample and returns mini batch matrices of both and number of batches\n",
    "    \"\"\"\n",
    "\n",
    "    # Batches must divide evenly into total number of samples for numpy arrays to be happy.\n",
    "    # Gets number of bathes by finding next smallest number that evenly divides\n",
    "    num_batches = start_batch_size\n",
    "    while len(x_sample) % num_batches != 0:\n",
    "        num_batches -= 1\n",
    "\n",
    "    # randomly shuffle indices\n",
    "    np.random.seed(42)\n",
    "    random_indices = np.random.choice(range(len(x_sample)), len(x_sample), replace=False)\n",
    "\n",
    "    # instantiate lists to hold batches\n",
    "    x_list = [[] for i in range(num_batches)]\n",
    "    y_list = [[] for i in range(num_batches)]\n",
    "\n",
    "    # populate batches matrix with random mini batch indices\n",
    "    for i in range(len(x_sample)):\n",
    "\n",
    "        x_list[i // 105].append(x_sample[random_indices[i]])\n",
    "        y_list[i // 105].append(y_sample[random_indices[i]])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_batch = np.array(x_list)\n",
    "    y_batch = np.array(y_list)\n",
    "\n",
    "    return x_batch, y_batch, num_batches, num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "For our first exercise we will implement plain old gradient descent. The mathematical formula is:\n",
    "\n",
    "$$ \\theta_t = \\theta_{t-1} - \\alpha \\nabla f(\\theta_{t-1}) \\tag{1}$$\n",
    "---\n",
    "\n",
    "We have already specified initial values for the $\\alpha$ and the batch size but feel free to play around with it. The location to insert your gradient descent implementation is outlined with a multi-line comment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.305485 Accuracy 0.095143\n",
      "Epoch 10 Loss 2.296722 Accuracy 0.095143\n",
      "Epoch 20 Loss 2.287348 Accuracy 0.095143\n",
      "Epoch 30 Loss 2.276909 Accuracy 0.128000\n",
      "Epoch 40 Loss 2.265160 Accuracy 0.189429\n",
      "Epoch 50 Loss 2.251949 Accuracy 0.233714\n",
      "Epoch 60 Loss 2.237195 Accuracy 0.373429\n",
      "Epoch 70 Loss 2.220911 Accuracy 0.504286\n",
      "Epoch 80 Loss 2.203200 Accuracy 0.572000\n",
      "Epoch 90 Loss 2.184210 Accuracy 0.613429\n",
      "Epoch 100 Loss 2.164103 Accuracy 0.633714\n",
      "Epoch 110 Loss 2.143041 Accuracy 0.636857\n",
      "Epoch 120 Loss 2.121186 Accuracy 0.637143\n",
      "Epoch 130 Loss 2.098684 Accuracy 0.636000\n",
      "Epoch 140 Loss 2.075672 Accuracy 0.636286\n",
      "Epoch 150 Loss 2.052271 Accuracy 0.636571\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-90fb4e4957f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mas\u001b[0m \u001b[0mwell\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0myour\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \"\"\"\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mgrad_W1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_W2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_b2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackpropogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mW1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad_W1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-6edc04e5c57c>\u001b[0m in \u001b[0;36mbackpropogate\u001b[1;34m(y_pred, Z1, X, y_obs, alpha_1, alpha_2)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mgrad_b2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mdelta_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta_2\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mZ1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mgrad_W1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mdelta_1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0malpha_1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mgrad_b1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Vanilla Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "vanilla_loss = []\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "        \n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        W1 -= eta*grad_W1\n",
    "        W2 -= eta*grad_W2\n",
    "        b1 -= eta*grad_b1\n",
    "        b2 -= eta*grad_b2\n",
    "\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    vanilla_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum Gradient Descent\n",
    "\n",
    "Next we will add momentum. The the new update rule is:\n",
    "\n",
    "$$m_{t}=\\beta_{1} m_{t-1} + (1 - \\beta_{1})\\nabla f(\\theta_{t-1})$$\n",
    "---\n",
    "\n",
    "$$ \\beta_1 \\in [0,1)$$\n",
    "---\n",
    "\n",
    "$$\\theta_{t}=\\theta_{t-1} - \\alpha m_{t} \\tag{2}$$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.305494 Accuracy 0.095143\n",
      "Epoch 10 Loss 2.296731 Accuracy 0.095143\n",
      "Epoch 20 Loss 2.287360 Accuracy 0.095143\n",
      "Epoch 30 Loss 2.276924 Accuracy 0.127714\n",
      "Epoch 40 Loss 2.265179 Accuracy 0.189429\n",
      "Epoch 50 Loss 2.251973 Accuracy 0.234000\n",
      "Epoch 60 Loss 2.237223 Accuracy 0.372286\n",
      "Epoch 70 Loss 2.220943 Accuracy 0.503714\n",
      "Epoch 80 Loss 2.203236 Accuracy 0.571714\n",
      "Epoch 90 Loss 2.184251 Accuracy 0.613429\n",
      "Epoch 100 Loss 2.164147 Accuracy 0.633714\n",
      "Epoch 110 Loss 2.143089 Accuracy 0.636571\n",
      "Epoch 120 Loss 2.121236 Accuracy 0.637143\n",
      "Epoch 130 Loss 2.098736 Accuracy 0.636000\n",
      "Epoch 140 Loss 2.075726 Accuracy 0.636286\n",
      "Epoch 150 Loss 2.052327 Accuracy 0.636571\n",
      "Epoch 160 Loss 2.028645 Accuracy 0.637714\n",
      "Epoch 170 Loss 2.004789 Accuracy 0.638571\n",
      "Epoch 180 Loss 1.980861 Accuracy 0.642000\n",
      "Epoch 190 Loss 1.956931 Accuracy 0.643429\n",
      "Epoch 200 Loss 1.933058 Accuracy 0.646000\n",
      "Epoch 210 Loss 1.909294 Accuracy 0.648857\n",
      "Epoch 220 Loss 1.885682 Accuracy 0.651714\n",
      "Epoch 230 Loss 1.862261 Accuracy 0.656000\n",
      "Epoch 240 Loss 1.839062 Accuracy 0.660286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Momentum Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add another hyper parameter here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "\n",
    "# data for analysis\n",
    "momentum_loss = []\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your momentum here... \n",
    "\"\"\"\n",
    "beta = .5\n",
    "M1 = np.zeros((n_0,n_1))\n",
    "M2 = np.zeros((n_1,N))\n",
    "Mb1 = np.zeros(n_1)\n",
    "Mb2 = np.zeros(N)\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        M1 = beta*M1 + (1-beta)*grad_W1 \n",
    "        M2 = beta*M2 + (1-beta)*grad_W2\n",
    "        Mb1 = beta*Mb1 + (1-beta)*grad_b1\n",
    "        Mb2 = beta*Mb2 + (1-beta)*grad_b2 \n",
    "        \n",
    "        W1 -= eta*M1\n",
    "        W2 -= eta*M2\n",
    "        b1 -= eta*Mb1\n",
    "        b2 -= eta*Mb2\n",
    "\n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    momentum_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp Gradient Descent \n",
    "\n",
    "Next we will do RMSProp. The update rule is as follows:\n",
    "\n",
    "$$v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\nabla f(\\theta_{t-1})^2$$\n",
    "---\n",
    "\n",
    "$$ \\beta_2 \\in [0,1)$$\n",
    "---\n",
    "\n",
    "$$\\theta_{t} = \\theta_{t-1} - \\alpha \\frac { \\nabla f(\\theta_{t-1})}{\\sqrt{v_{t} + \\epsilon}} \\tag{3}$$ \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.190878 Accuracy 0.555429\n",
      "Epoch 10 Loss 1.412134 Accuracy 0.865429\n",
      "Epoch 20 Loss 0.894033 Accuracy 0.888857\n",
      "Epoch 30 Loss 0.596558 Accuracy 0.902571\n",
      "Epoch 40 Loss 0.430691 Accuracy 0.907429\n",
      "Epoch 50 Loss 0.333946 Accuracy 0.912286\n",
      "Epoch 60 Loss 0.273054 Accuracy 0.917714\n",
      "Epoch 70 Loss 0.231462 Accuracy 0.920571\n",
      "Epoch 80 Loss 0.201149 Accuracy 0.923714\n",
      "Epoch 90 Loss 0.177896 Accuracy 0.926000\n",
      "Epoch 100 Loss 0.159563 Accuracy 0.929143\n",
      "Epoch 110 Loss 0.144549 Accuracy 0.929429\n",
      "Epoch 120 Loss 0.132066 Accuracy 0.930571\n",
      "Epoch 130 Loss 0.121486 Accuracy 0.931714\n",
      "Epoch 140 Loss 0.112380 Accuracy 0.932571\n",
      "Epoch 150 Loss 0.104411 Accuracy 0.932857\n",
      "Epoch 160 Loss 0.097409 Accuracy 0.934286\n",
      "Epoch 170 Loss 0.091203 Accuracy 0.934286\n",
      "Epoch 180 Loss 0.085590 Accuracy 0.933714\n",
      "Epoch 190 Loss 0.080617 Accuracy 0.933429\n",
      "Epoch 200 Loss 0.076156 Accuracy 0.933429\n",
      "Epoch 210 Loss 0.072105 Accuracy 0.932286\n",
      "Epoch 220 Loss 0.068348 Accuracy 0.932286\n",
      "Epoch 230 Loss 0.064923 Accuracy 0.932286\n",
      "Epoch 240 Loss 0.061837 Accuracy 0.933429\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RMSProp Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-5\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add two hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "RMS_loss = []\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your variance here...\n",
    "\"\"\"\n",
    "beta2 = .9\n",
    "epsilon = 1e-8\n",
    "vW1 = np.zeros((n_0,n_1))\n",
    "vW2 = np.zeros((n_1,N))\n",
    "vb1 = np.zeros(n_1)\n",
    "vb2 = np.zeros(N)\n",
    "\n",
    "# Perform gradient descent\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "\n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "        vW1 = vW1*beta2 + (1-beta2)*(grad_W1**2)\n",
    "        vW2 = vW2*beta2 + (1-beta2)*(grad_W2**2)\n",
    "        vb1 = vb1*beta2 + (1-beta2)*(grad_b1**2)\n",
    "        vb2 = vb2*beta2 + (1-beta2)*(grad_b2**2)\n",
    "        \n",
    "        W1 -= eta*grad_W1/np.sqrt(vW1 + epsilon)\n",
    "        W2 -= eta*grad_W2/np.sqrt(vW2 + epsilon)\n",
    "        b1 -= eta*grad_b1/np.sqrt(vb1 + epsilon)\n",
    "        b2 -= eta*grad_b2/np.sqrt(vb2 + epsilon)\n",
    "\n",
    "       \n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    RMS_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.plot(RMS_loss, label='RMSProp')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam Gradient Descent \n",
    "\n",
    "Now we put them both together and we get Adam!\n",
    "\n",
    "$$m_{t}=\\beta_{1} m_{t-1} + (1 - \\beta_{1})\\nabla f(\\theta_{t-1})$$\n",
    "---\n",
    "\n",
    "$$v_{t} = \\beta_{2}v_{t-1} + (1 - \\beta_{2})\\nabla f(\\theta_{t-1})^2$$\n",
    "---\n",
    "\n",
    "$$\\hat m_{t} = \\frac {m_{t}}{1 - \\beta_1^{t}}$$\n",
    "---\n",
    "\n",
    "$$\\hat v_t = \\frac {v_{t}}{1 - \\beta_2^{t}}$$\n",
    "---\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha \\frac {\\hat m_t}{\\sqrt{\\hat v_t + \\epsilon}} \\tag{4}$$\n",
    "---\n",
    "\n",
    "Play around with the hyperparameters to see if you can get distinctly different behavior from the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 2.199524 Accuracy 0.460286\n",
      "Epoch 10 Loss 1.411599 Accuracy 0.876000\n",
      "Epoch 20 Loss 0.900433 Accuracy 0.898000\n",
      "Epoch 30 Loss 0.602088 Accuracy 0.908857\n",
      "Epoch 40 Loss 0.432437 Accuracy 0.916286\n",
      "Epoch 50 Loss 0.331789 Accuracy 0.922857\n",
      "Epoch 60 Loss 0.267299 Accuracy 0.925714\n",
      "Epoch 70 Loss 0.223130 Accuracy 0.929143\n",
      "Epoch 80 Loss 0.191206 Accuracy 0.929429\n",
      "Epoch 90 Loss 0.166953 Accuracy 0.930286\n",
      "Epoch 100 Loss 0.147634 Accuracy 0.929714\n",
      "Epoch 110 Loss 0.132000 Accuracy 0.930857\n",
      "Epoch 120 Loss 0.119165 Accuracy 0.931143\n",
      "Epoch 130 Loss 0.108405 Accuracy 0.932286\n",
      "Epoch 140 Loss 0.099183 Accuracy 0.932000\n",
      "Epoch 150 Loss 0.091270 Accuracy 0.932571\n",
      "Epoch 160 Loss 0.084350 Accuracy 0.934000\n",
      "Epoch 170 Loss 0.078212 Accuracy 0.933714\n",
      "Epoch 180 Loss 0.072878 Accuracy 0.933429\n",
      "Epoch 190 Loss 0.068158 Accuracy 0.932857\n",
      "Epoch 200 Loss 0.063935 Accuracy 0.933714\n",
      "Epoch 210 Loss 0.060178 Accuracy 0.933714\n",
      "Epoch 220 Loss 0.056706 Accuracy 0.934000\n",
      "Epoch 230 Loss 0.053560 Accuracy 0.933714\n",
      "Epoch 240 Loss 0.050687 Accuracy 0.933143\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adam Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 1e-3\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add 3 hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "epsilon = 1e-8\n",
    "beta1 = .9\n",
    "beta2 = .99\n",
    "\n",
    "vW1 = np.zeros((n_0,n_1))\n",
    "vW2 = np.zeros((n_1,N))\n",
    "vb1 = np.zeros(n_1)\n",
    "vb2 = np.zeros(N)\n",
    "\n",
    "M1 = np.zeros((n_0,n_1))\n",
    "M2 = np.zeros((n_1,N))\n",
    "Mb1 = np.zeros(n_1)\n",
    "Mb2 = np.zeros(N)\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "Adam_loss = []\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your variance and momentum here...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "count = 1\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "        \n",
    "        M1 = beta1*M1 + (1-beta1)*grad_W1 \n",
    "        M2 = beta1*M2 + (1-beta1)*grad_W2\n",
    "        Mb1 = beta1*Mb1 + (1-beta1)*grad_b1\n",
    "        Mb2 = beta1*Mb2 + (1-beta1)*grad_b2\n",
    "        \n",
    "        vW1 = vW1*beta2 + (1-beta2)*(grad_W1**2)\n",
    "        vW2 = vW2*beta2 + (1-beta2)*(grad_W2**2)\n",
    "        vb1 = vb1*beta2 + (1-beta2)*(grad_b1**2)\n",
    "        vb2 = vb2*beta2 + (1-beta2)*(grad_b2**2)\n",
    "        \n",
    "        m1_hat = M1/(1-beta1**count)\n",
    "        m2_hat = M2/(1-beta1**count)\n",
    "        mb1_hat = Mb1/(1-beta1**count)\n",
    "        mb2_hat = Mb2/(1-beta1**count)\n",
    "        \n",
    "        v1_hat = vW1/(1-beta2**count)\n",
    "        v2_hat = vW2/(1-beta2**count)\n",
    "        vb1_hat = vb1/(1-beta2**count)\n",
    "        vb2_hat = vb2/(1-beta2**count)\n",
    "        \n",
    "        W1 -= eta*m1_hat/np.sqrt(v1_hat + epsilon)\n",
    "        W2 -= eta*m2_hat/np.sqrt(v2_hat + epsilon)\n",
    "        b1 -= eta*mb1_hat/np.sqrt(vb1_hat + epsilon)\n",
    "        b2 -= eta*mb2_hat/np.sqrt(vb2_hat + epsilon)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    Adam_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.plot(RMS_loss, label='RMSProp')\n",
    "plt.plot(Adam_loss, label='Adam')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.624618 Accuracy 0.890571\n",
      "Epoch 10 Loss 0.120499 Accuracy 0.940286\n",
      "Epoch 20 Loss 0.058756 Accuracy 0.949143\n",
      "Epoch 30 Loss 0.032706 Accuracy 0.950286\n",
      "Epoch 40 Loss 0.021468 Accuracy 0.952571\n",
      "Epoch 50 Loss 0.014572 Accuracy 0.954286\n",
      "Epoch 60 Loss 0.010510 Accuracy 0.954000\n",
      "Epoch 70 Loss 0.009532 Accuracy 0.953714\n",
      "Epoch 80 Loss 0.009855 Accuracy 0.951429\n",
      "Epoch 90 Loss 0.010370 Accuracy 0.953143\n",
      "Epoch 100 Loss 0.020701 Accuracy 0.950000\n",
      "Epoch 110 Loss 0.009277 Accuracy 0.956000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "AdaMax Gradient Descent\n",
    "\"\"\"\n",
    "\n",
    "# Hyper Parameters\n",
    "eta = 2e-3\n",
    "initial_batch_size = 104\n",
    "epochs = 250\n",
    "\"\"\"\n",
    "You need to add 3 hyper parameters here. Hint! look at the equation above\n",
    "\"\"\"\n",
    "epsilon = 1e-8\n",
    "beta1 = .9\n",
    "beta2 = .99\n",
    "\n",
    "vW1 = 0\n",
    "vW2 = 0\n",
    "vb1 = 0\n",
    "vb2 = 0\n",
    "\n",
    "M1 = np.zeros((n_0,n_1))\n",
    "M2 = np.zeros((n_1,N))\n",
    "Mb1 = np.zeros(n_1)\n",
    "Mb2 = np.zeros(N)\n",
    "\n",
    "# Initialize random parameter matrices\n",
    "np.random.seed(42)\n",
    "W1 = 0.001 * np.random.randn(n_0, n_1)\n",
    "W2 = 0.001 * np.random.randn(n_1, N)\n",
    "\n",
    "b1 = 0.1 * np.random.randn(1, n_1)\n",
    "b2 = 0.1 * np.random.randn(1, N)\n",
    "\n",
    "# data for analysis\n",
    "Ada_max_loss = []\n",
    "\n",
    "\"\"\"\n",
    "You probably need to initialize your variance and momentum here...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Perform gradient descent\n",
    "count = 1\n",
    "for i in range(epochs):\n",
    "\n",
    "    # generate mini batches\n",
    "    x_batches, y_batches, num_batches, actual_batch_size = mini_batch(X, one_hot_y_actual, initial_batch_size)\n",
    "\n",
    "    # perform gradient descent on mini batches\n",
    "    for j in range(num_batches):\n",
    "        y_pred, Z1 = feedforward(x_batches[j], W1, W2, b1, b2)\n",
    "\n",
    "        \"\"\"\n",
    "        These are your gradients with respect to weight matrices W1 and W2 \n",
    "        as well as your biases b1 and b2\n",
    "        \"\"\"\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backpropogate(y_pred, Z1, x_batches[j], y_batches[j])\n",
    "        \n",
    "        M1 = beta1*M1 + (1-beta1)*grad_W1 \n",
    "        M2 = beta1*M2 + (1-beta1)*grad_W2\n",
    "        Mb1 = beta1*Mb1 + (1-beta1)*grad_b1\n",
    "        Mb2 = beta1*Mb2 + (1-beta1)*grad_b2\n",
    "        \n",
    "        vW1 = np.max([beta2*vW1,np.max(np.abs(grad_W1))])\n",
    "        vW2 = np.max([beta2*vW2,np.max(np.abs(grad_W2))])\n",
    "        vb1 = np.max([beta2*vb1,np.max(np.abs(grad_b1))])\n",
    "        vb2 = np.max([beta2*vb2,np.max(np.abs(grad_b2))])\n",
    "        \n",
    "      \n",
    "        \n",
    "        W1 -= (eta*M1/vW1)/(1-beta1**count)\n",
    "        W2 -= (eta*M2/vW2)/(1-beta1**count)\n",
    "        b1 -= (eta*Mb1/vb1)/(1-beta1**count)\n",
    "        b2 -= (eta*Mb2/vb2)/(1-beta1**count)\n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        You put your code here. W1 and W2 are your weight matrices.\n",
    "        b1 and b2 are your bias for each matrix.\n",
    "        Each are numpy arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "    # calc loss at end of each epoch\n",
    "    y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "    Ada_max_loss.append(L(one_hot_y_actual, y_entire_pred, W1, W2))\n",
    "\n",
    "    # Print some summary statistics every ten iterations\n",
    "    if i % 10 == 0:\n",
    "        y_pred_test, Z1_test = feedforward(X_test, W1, W2, b1, b2)\n",
    "        acc = sum(y_test == np.argmax(y_pred_test, axis=1)) / len(y_test)\n",
    "        y_entire_pred, Z1 = feedforward(X, W1, W2, b1, b2)\n",
    "        print(\"Epoch %d Loss %f Accuracy %f\" % (i, L(one_hot_y_actual, y_entire_pred, W1, W2), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vanilla_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c9cb0f49c430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvanilla_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Vanilla'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Momentum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRMS_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'RMSProp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vanilla_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "plt.plot(vanilla_loss, label='Vanilla')\n",
    "plt.plot(momentum_loss, label='Momentum')\n",
    "plt.plot(RMS_loss, label='RMSProp')\n",
    "plt.plot(Adam_loss, label='Adam')\n",
    "plt.plot(Ada_max_loss,label='Ada-max')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
